# Завдання для студентів-практикантів: Генерація розгорнутого опису лістингу за допомогою мовних моделей

## Опис завдання
1. **Створити AWS Lambda-функцію**, яка отримує JSON-дані про лістинг (інформація про нерухомість).  
2. На базі цих даних (і за можливості — фото) **згенерувати розгорнутий опис** лістингу, використовуючи **зовнішню мовну модель**.  
3. Є **два рівні складності**:
   - **Простий варіант**: Використати **OpenAI API (ChatGPT)**.
   - **Складний варіант**: **Розгорнути open-source модель** (наприклад, [Ollama](https://github.com/jmorganca/ollama), [DeepSeek](https://github.com/deepseeklabs/deepseek) чи іншу) в Docker-контейнері, підключити її до AWS Lambda та забезпечити можливість запуску локально.

---

## Основні вимоги

1. **Вхідні дані**  
   - Формат: **JSON**, що містить усю потрібну інформацію про лістинг (наприклад: назва, опис, характеристики, зображення, ціна тощо).  
   - Файл з прикладом знаходиться в корені репозиторію

2. **Вихідні дані**  
   - Розгорнутий, **згенерований текст** на основі лістингу.  
   - За бажанням, можна також включити **короткий підсумок** .

3. **Використання мовної моделі**  
   - **Простий варіант**: Виклик API ChatGPT (OpenAI) з відповідними параметрами (`prompt`, `temperature`, `max_tokens` тощо).  
   - **Складний варіант**: Розгорнути open-source модель у Docker. Забезпечити, щоб:
     - Модель мала доступ до JSON-даних.
     - Код AWS Lambda викликав цю модель локально або через HTTP/gRPC.  
     - Модель могла бути повторно використана не тільки в Lambda, але й локально.

4. **Мова програмування**  
   - Бажано **Python**, але можна використовувати й іншу за потреби.

5. **Структура коду**  
   - Файл(и) для AWS Lambda:
     - Обробка вхідного запиту (JSON).
     - Виклик обраної мовної моделі (або через HTTP-запит, або безпосередньо).
     - Формування та повернення результату.  
   - **Dockerfile** (для складного варіанту):
     - Встановлення потрібних залежностей (open-source модель, бібліотеки).
     - Налаштування сервісу для взаємодії з моделлю.
   - Приклади **скриптів або команд** для локального запуску (складний варіант).

---

## Рекомендована послідовність роботи

1. **Ознайомлення з AWS Lambda**  
   - Зрозуміти структуру лямбда-функції: вхід (event, context), вихід (json).
   - Встановити та налаштувати AWS CLI (за потреби).
   - Можна використати імітацію AWS: https://docs.localstack.cloud/user-guide/aws/lambda/

2. **Налаштування середовища**  
   - Створити окреме середовище Python (virtualenv / venv) або використати готовий Docker-контейнер.
   - Встановити необхідні бібліотеки: `requests`, `boto3` (якщо потрібно), `openai` (для простого варіанту), а також залежності для open-source моделі (для складного).

3. **Розробка "Простого варіанта"**  
   - Створити лямбда-функцію (наприклад, `lambda_function.py`), яка:
     1. Зчитує JSON з `event` (наприклад, `event['body']`).
     2. Викликає OpenAI API з відповідним `prompt`, куди підставляє інформацію з лістингу.
     3. Форматує результат і повертає розгорнутий опис у JSON-відповіді.
   - Переконатися, що все працює на тестових даних.

4. **Розробка "Складного варіанта"**  
   - **Розгорнути open-source модель** в Docker:
     1. Вибрати модель (Ollama / DeepSeek / іншу).  
     2. Створити Dockerfile, що встановлює потрібні залежності.  
     3. Запустити контейнер зі сервісом, що приймає вхідний текст та повертає згенеровану відповідь.
   - **Інтегрувати Lambda** з цим сервісом:
     1. Використати AWS Lambda Layers або ECR (Amazon Elastic Container Registry), якщо це потрібно.  
     2. Переконатися, що Lambda може викликати контейнер (локально або через мережу).
   - Забезпечити **локальний запуск** (через `docker run` або скрипт, що надсилає запит на контейнер) для зручного розроблення без постійного деплою в AWS.

5. **Тестування**  
   - Використати **приклади JSON** для лістингів різної складності.  
   - Перевірити, як модель обробляє описи різної довжини, чи коректно вставляє ключові деталі з JSON.  
   - Перевірити швидкість відповіді та загальний обсяг результату (можливо, варто коригувати `max_tokens` чи інші параметри).

6. **Документація**  
   - Описати, як налаштувати та запустити проєкт (додати інструкції у README).
   - Додати приклади вхідних/вихідних даних.
   - Вказати, де зберігаються Docker-образи (у складному варіанті).

---

## Додаткові рекомендації

- Використовуйте **змінні оточення (env)** для зберігання конфіденційних даних (API-ключі тощо).
- Якщо обсяг тексту великий, зверніть увагу на **ліміти AWS Lambda** (як за часом, так і за пам’яттю).

---

## Очікувані результати

1. **Репозиторій (GitHub, GitLab чи інший)**, що містить:
   - Код лямбда-функції.
   - Файл(и) для інтеграції з моделлю (простий/складний варіант).
   - Документацію (README з інструкціями з налаштування).
   - Приклади JSON-вхідних даних.
   - (За потреби) Dockerfile для моделі у складному варіанті.

2. **Демонстрація роботи**:
   - Тестовий виклик Lambda (через AWS Console або CLI).
   - Для складного варіанту — також локальний виклик Docker-контейнера.

---

**Бажаємо успіху в реалізації!**  
> *Пам’ятайте, що головна мета завдання — навчитися запускати мовні моделі в середовищі AWS Lambda (як через офіційне API, так і через власні контейнеризовані рішення).*
